{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import flat_to_triangular, make_cholesky, plot, clean_curr_axes, onehot, log_sum_exp\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $N$-dimensional features, scalar regression target, with covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "We consider a egression problem with N-dimensional features $x$, scalar targets $t$ and known variance $\\sigma^2$ We wish to learn a bayesian linear model with learnt basis functions. \n",
    "\n",
    "### Case of the known basis function\n",
    "\n",
    "In the case of a known deterministic N-dimensional basis function $\\phi$, le likelihood is expressed as:\n",
    "\n",
    "\\begin{align}\n",
    "p(t | a, b, x, \\sigma) & = \\mathcal{N}(a \\phi(x) + b, \\sigma^2) \n",
    "\\end{align}\n",
    "\n",
    "We define the prior over the parameters as an isotropic Gaussian, which gives:\n",
    "\n",
    "\\begin{align}\n",
    " p(a) = \\mathcal{N}(0,1) \\qquad  p(b) = \\mathcal{N}(0,1)\n",
    "\\end{align}\n",
    "\n",
    "It is then possible to compute the posterior of parameters analytically, given $(\\mathbf{x}, \\mathbf{t})$ the entire dataset:\n",
    "\n",
    "\\begin{align}\n",
    "p(a,b |\\mathbf{t} , \\mathbf{x}, \\sigma^2) = \\frac{p(\\mathbf{t} | a, b, \\mathbf{x}, \\sigma^2) p(a,b)}{p(\\mathbf{t}, \\mathbf{x})}\n",
    "\\end{align}\n",
    "\n",
    "$p(\\mathbf{t})$ is tractable and constant, hence we can also approximate the posterior using our favorite MCMC methods.\n",
    "\n",
    "\n",
    "### Learnt basis function\n",
    "\n",
    "#### Bayes by Backprop\n",
    "\n",
    "We propose to optimize $\\theta$ using maximum likelihood and $a, b$ using Variational Inference, which allows us deriving a lower bound of the marginal log likelihood (with regards to $a,b$) that we can optimize end-to-end. We have:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{t} | \\mathbf{x}, \\sigma^2, \\theta) & = \\log \\int_{a,b} \\log p(\\mathbf{t}, a, b, | \\mathbf{x}, \\sigma^2, \\theta) dadb \\\\\n",
    "& = \\log \\int_{a,b} \\log p(\\mathbf{t} | a, b, \\mathbf{x}, \\sigma^2, \\theta)p(a,b) dadb \\\\\n",
    "& = \\log \\int_{a,b} q(a,b) p(\\mathbf{t} | a, b, \\mathbf{x}, \\sigma^2, \\theta) \\frac{p(a,b)}{q(a,b)} dadb \\\\\n",
    "& \\geq \\mathbb{E}_{q(a,b)} \\left[ \\log  p(\\mathbf{t} | a, b, \\mathbf{t}, \\sigma^2, \\theta) \\frac{p(a,b)}{q(a,b)} \\right] \\\\\n",
    "& = \\mathbb{E}_q[\\log p_{\\theta}(\\mathbf{t}\\ |\\ a, b, \\mathbf{x})] - \\mathrm{KL}(q(a,b) \\ ||\\ p(a,b)) \\\\\n",
    "= \\mathcal{L_(t, x)}\n",
    "\\end{align}\n",
    "\n",
    "#### Modelling the posterior of vector quantities with covariance\n",
    "\n",
    "In this context, $a$ is a vector of size $N$ and $b$ a scalar. We write $v$ the vector representing all the parameters ${a_i}_a + b$. In this context, we choose to model the approximate posterior of $v$ using a **Multivariate Gaussian** parametrized by a Cholesky factor $L$, which allows us to apply the reparametrization trick [[Pathwise Derivatives Beyond the Reparameterization Trick]](https://arxiv.org/pdf/1806.01851.pdf):\n",
    "\n",
    "\\begin{align}\n",
    "y \\sim \\mathcal{N}(\\mu, L) \\\\\n",
    "y = \\mu + L \\epsilon \\\\\n",
    "\\epsilon \\sim \\mathcal{N}(\\mu, I)\n",
    "\\end{align}\n",
    "\n",
    "The covariance matrix is then defined as:\n",
    "\n",
    "\\begin{aligned} \\boldsymbol{\\Sigma} &=\\mathbb{E}\\left[(\\mathbf{z}-\\mathbb{E}[\\mathbf{z} |])(\\mathbf{z}-\\mathbb{E}[\\mathbf{z} |])^{T}\\right] \\\\ &=\\mathbb{E}\\left[\\mathbf{L} \\boldsymbol{\\epsilon}(\\mathbf{L} \\boldsymbol{\\epsilon})^{T}\\right] \\\\ &=\\mathbb{E}\\left[\\mathbf{L} \\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^{T} \\mathbf{L}^{T}\\right] \\\\ &=\\mathbf{L} \\mathbb{E}\\left[\\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^{T}\\right] \\mathbf{L}^{T} \\\\ &=\\mathbf{L} \\mathbf{L}^{T} \\end{aligned}\n",
    "\n",
    "\n",
    "We can then write our ELBO as:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbb{E}_q[\\log p_{\\theta}(\\mathbf{t}\\ |\\ a, b, \\mathbf{x})] =  \\mathbb{E}_q \\left[ \\sum_{x \\in \\mathbf{x}} \\left[ -\\frac{1}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} [(t - a f_{\\theta}(x) - b)^2] + \\mathrm{const.} \\right] \\right] \\\\\n",
    "\\mathrm{KL}(q(v) \\ ||\\ p(v)) = -\\frac{1}{2} \\sum_{i\\in v} \\left(1 + \\log L_{i,i}^2 - \\mu_i^2 - L_{i,i}^2\\right)\n",
    "\\end{split}\n",
    "\n",
    "**TODO: check if KL correct**\n",
    "\n",
    "\n",
    "### Papers\n",
    "\n",
    "[Gaussian variational approximation with a factor covariance structure](https://arxiv.org/pdf/1701.03208.pdf)\n",
    "\n",
    "[Pathwise Derivatives Beyond the Reparameterization Trick](https://arxiv.org/pdf/1806.01851.pdf)\n",
    "\n",
    "[Berkley Kingma ppt](https://berkeley-deep-learning.github.io/cs294-131-s17/slides/VAE%20talk.compressed.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 150\n",
    "# sigma = 0.2\n",
    "# n_classes = 3\n",
    "# true_a = np.array([0.6, 1, .3]).reshape(-1, 1)\n",
    "# true_b = 0.2\n",
    "# # x = np.linspace(-1, 1, N).reshape(-1, 1)\n",
    "# x = np.random.rand(N, 1) * 2 - 1\n",
    "# x = np.sort(x, axis=0)\n",
    "# fx1 = x - 1\n",
    "# fx2 = x**2\n",
    "# fx3 = np.sin(12 * x)\n",
    "# fx = np.concatenate((fx1, fx2, fx3), axis=-1)\n",
    "# tmp = fx @ true_a + true_b + sigma*np.random.randn(*x.shape)  # function to compute labels\n",
    "\n",
    "# # Divide data interval in n_classes regions\n",
    "# # thresholds = np.linspace(tmp.min() - 1e-3, tmp.max() + 1e-3, n_classes+1)  # equal size\n",
    "# thresholds = np.percentile(tmp, np.linspace(0, 100, n_classes+1))  # balanced classes\n",
    "\n",
    "# # targets\n",
    "# t = np.zeros((tmp.shape[0], n_classes), dtype=int)  # one-hot, shape (N, C)\n",
    "# for i in range(n_classes):\n",
    "#     t[:, i] = ((thresholds[i] < tmp) * (thresholds[i+1] >= tmp))[:, 0]\n",
    "# t_values = t * np.arange(n_classes).reshape((1, -1))\n",
    "# t_values = np.sum(t_values, 1)\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.plot(x, fx @ true_a + true_b)\n",
    "# plt.plot(x, fx, alpha=0.5)\n",
    "# sns.scatterplot(x[:, 0], tmp[:, 0])\n",
    "# for i in range(1, n_classes):\n",
    "#     plt.plot([x[0], x[-1]], np.array([1., 1.]) * thresholds[i], color='k', alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "# sns.scatterplot(x[:, 0], t_values)\n",
    "# plt.ylabel('class')\n",
    "# plt.show()\n",
    "\n",
    "# print('x shape:', x.shape)\n",
    "# print('f shape:', fx.shape)\n",
    "# print('true_a shape:', true_a.shape)\n",
    "# print('t shape:', t.shape)\n",
    "# print('t_values shape:', t_values.shape)\n",
    "\n",
    "# x = torch.tensor(x, dtype=torch.float)\n",
    "# t_values = torch.tensor(t_values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "n_classes = 10\n",
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features extractor and stochastic layer\n",
    "\n",
    "Define function $f_{\\theta}$ to extract features $\\mathbf{h} = f_{\\theta}(x)$, with parameters $\\theta$ to be optimized. The features $\\mathbf{h}$ have size $m$, the weight matrix $\\mathbf{a}$ has shape $m \\times k$, the bias $\\mathbf{b}$ has size $k$, and $k$ is the number of classes. Logits are $\\mathbf{h \\cdot a} + \\mathbf{b}$, with $\\mathbf{a}$ and $\\mathbf{b}$ multivariate Normal rvs with full covariance (including correlation between $\\mathbf{a}$ and $\\mathbf{b}$).\n",
    "\n",
    "The stochastic layer has no state, it's only used to get samples from $\\mathcal{N}(\\mu, \\Sigma)$, and to compute the KL divergence from a prior $\\mathcal{N}(0, I)$ to a variational distribution $q = \\mathcal{N}(\\mu, \\Sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractor(nn.Module):\n",
    "    def __init__(self, n_out):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, stride=2), nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, 5, stride=2), nn.LeakyReLU(),\n",
    "        )\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(4*4*64, 256), nn.LeakyReLU(),\n",
    "            nn.Linear(256, n_out), nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fcs(x)\n",
    "    \n",
    "    \n",
    "class CovarianceStochasticLayer(nn.Module):\n",
    "    def __init__(self, deterministic=False):\n",
    "        super().__init__()\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def forward(self, mu_q, cholesky):\n",
    "        if self.deterministic:\n",
    "            return mu_q\n",
    "        else:\n",
    "            eps = torch.randn_like(mu_q)\n",
    "            return mu_q + eps @ cholesky\n",
    "    \n",
    "    def forward_diagonal(self, mu_q, logvar_q):\n",
    "        eps = torch.randn_like(mu_q)\n",
    "        std = (logvar_q/2).exp()\n",
    "        return mu_q + std * eps\n",
    "\n",
    "    def get_kl(self, mu_q, logvar_q):\n",
    "        \"\"\"KL from standard normal. Shape is irrelevant, KL is summed over all dimensions.\"\"\"\n",
    "        if not self.deterministic:\n",
    "            kl = -0.5 * (1 + logvar_q - mu_q.pow(2) - logvar_q.exp())\n",
    "            return kl.sum()\n",
    "        else:\n",
    "            0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "\n",
    "Here we maximize the ELBO $\\mathcal{L}$ with respect to variational parameters $\\mu_a, \\sigma_a, \\mu_b, \\sigma_b$, the NN parameters $\\theta$, and the noise std $\\sigma$.\n",
    "\n",
    "##### TODO: compensate for batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200 : loss 1.954 ELBO -1.954  Reconstruction 4.304843425750732  KL 0.005624102894216776   acc 0.151\n",
      "step 400 : loss 1.457 ELBO -1.457  Reconstruction 7.5794525146484375  KL 0.005631292704492807   acc 0.233\n",
      "step 600 : loss 1.270 ELBO -1.270  Reconstruction 8.499279022216797  KL 0.00564145902171731   acc 0.284\n",
      "step 800 : loss 1.115 ELBO -1.115  Reconstruction 7.411723613739014  KL 0.005651971325278282   acc 0.336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "last_layer_size = 32\n",
    "a_shape = (last_layer_size, n_classes)  # shape of stochastic weight matrix\n",
    "a_size = np.prod(a_shape)\n",
    "b_size = n_classes\n",
    "\n",
    "features = FeaturesExtractor(n_out=last_layer_size).to(device)\n",
    "stochastic = CovarianceStochasticLayer().to(device)\n",
    "l_rate = 1e-4\n",
    "epochs = 1000\n",
    "n_samples = 10  # MC samples per example\n",
    "n_parameters = a_size + b_size\n",
    "mu_ab = nn.Parameter(torch.zeros((n_parameters), device=device))  # for a and b\n",
    "\n",
    "# diagonal covariance\n",
    "logvar_ab = nn.Parameter(-3 * torch.ones((n_parameters), device=device))\n",
    "\n",
    "# off-diagonal covariance\n",
    "cov_ab_params = nn.Parameter(torch.zeros((n_parameters*(n_parameters-1)//2), device=device).normal_() * 0.01)\n",
    "\n",
    "# print(\"Cholesky:\\n\", make_cholesky(logvar_ab, cov_ab))\n",
    "\n",
    "optimiser = torch.optim.Adam(list(features.parameters()) + [mu_ab, logvar_ab, cov_ab_params], lr = l_rate)\n",
    "\n",
    "\n",
    "step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    features.train()\n",
    "    for batch_idx, (x, t) in enumerate(train_loader):\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        optimiser.zero_grad()\n",
    "        h = features(x)\n",
    "        cov_ab = flat_to_triangular(cov_ab_params)\n",
    "        \n",
    "        # sample n_samples times a and b\n",
    "        mu = mu_ab.unsqueeze(0).expand(n_samples, -1)\n",
    "        L = make_cholesky(logvar_ab, cov_ab)\n",
    "        ab_sample = stochastic(mu, L) \n",
    "        a, b = ab_sample[:, :-b_size], ab_sample[:, -b_size:]\n",
    "        \n",
    "        # reshape into [n_samples, *]\n",
    "        a = a.unsqueeze(1).view(-1, *a_shape)\n",
    "        b = b.unsqueeze(1)\n",
    "\n",
    "        # compute y of size [n_samples x batch_size x n_classes]\n",
    "        y = h @ a + b   # stochasticity here only comes from a and b\n",
    "        \n",
    "        # extend t and flatten \n",
    "        t = t.unsqueeze(0).expand(n_samples, batch_size)\n",
    "        t = t.contiguous().view(-1)\n",
    "        y = y.contiguous().view(-1, n_classes)\n",
    "        \n",
    "        # Reconstruction loss, estimated by MC\n",
    "        recons = f.cross_entropy(y, t, reduction='none')\n",
    "\n",
    "        # Accuracy\n",
    "        pred_class = torch.max(y.detach(), dim=1)[1]   # tensor of indices\n",
    "        acc = torch.mean((t==pred_class).float()).item()\n",
    "\n",
    "        # KL loss, both analytic and by MC\n",
    "        kl = stochastic.get_kl(mu_ab, logvar_ab) / len(train_loader.dataset)\n",
    "        \n",
    "        \n",
    "        # compute elbo and view as [n_samples x batch_size]\n",
    "        elbo = - recons - kl\n",
    "        elbo.view(n_samples, batch_size)\n",
    "\n",
    "        # IW averaging\n",
    "        #elbo = elbo.mean() # \n",
    "        elbo = log_sum_exp(elbo, dim=0, sum_op=torch.mean)\n",
    "\n",
    "        loss = - elbo\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        step += 1\n",
    "        if step % 200 == 0:\n",
    "            print(f'step {step} : loss {loss.item():.3f} ELBO {elbo.item():.3f}  Reconstruction '\n",
    "                  f'{recons.mean().item()}  KL {kl.item()}   acc {acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "First we sample $a,b \\sim q(a,b)$ according to the learned variational parameters. Get $N$ samples $a_i$ and $b_i$. Then sample from predictive distribution\n",
    "$$t_i = a_i f_{\\theta}(x_i) + b_i + \\sigma \\epsilon_i$$\n",
    "with $\\epsilon_i$ iid samples from $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get classifier output for the whole dataset and for many samples of the last layer's weights\n",
    "n_samples = 50   \n",
    "N = 200   # TODO the following stuff is computed only on N samples of training set!!\n",
    "with torch.no_grad():\n",
    "    # Compute deterministic NN output\n",
    "    x = torch.cat([train_loader.dataset[i][0].unsqueeze(0) for i in range(N)], dim=0)\n",
    "    t_values = torch.cat([train_loader.dataset[i][1].unsqueeze(0) for i in range(N)])\n",
    "    h = features(x)\n",
    "    cov_ab = flat_to_triangular(cov_ab_params)\n",
    "    y_all = []\n",
    "    for i in range(n_samples):\n",
    "        # Sample last layer's weights from posterior q(a, b)\n",
    "        ab_sample = stochastic(mu_ab, make_cholesky(logvar_ab, cov_ab))\n",
    "        a, b = ab_sample[:-b_size], ab_sample[-b_size:]\n",
    "        a = a.view(a_shape)\n",
    "        b = b.unsqueeze(0)\n",
    "        \n",
    "        # Compute last layer output (the logits) with the weights just sampled\n",
    "        y = h @ a + b    # shape: (N, n_classes)\n",
    "        \n",
    "        y_all.append(y.softmax(1).unsqueeze(0).cpu().numpy())\n",
    "\n",
    "y_all = np.concatenate(y_all, axis=0)  # shape: (n_samples, N, n_classes)\n",
    "\n",
    "\n",
    "### Get predictions (argmax) for all samples of weights (a,b)\n",
    "pred_class = np.argmax(y_all, axis=-1)   # tensor of indices, shape (n_samples, N)\n",
    "pred_class_onehot = onehot(pred_class)\n",
    "pred_class_mean = pred_class_onehot.mean(0)   # shape (N, n_classes)\n",
    "# pred_class_mean: for each input point a vector of size n_classes that sums to 1\n",
    "\n",
    "\n",
    "### Get confusion matrix based on those samples\n",
    "conf_matrix = np.zeros((n_classes, n_classes))\n",
    "for i in range(N):\n",
    "    conf_matrix[t_values[i]] += pred_class_mean[i]\n",
    "conf_matrix /= conf_matrix.sum(1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "### Show confusion matrix\n",
    "plt.imshow(conf_matrix)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "clean_curr_axes()\n",
    "plt.title('Confusion matrix (rows sum to 1)')\n",
    "plt.xlabel('Predicted top class')\n",
    "plt.ylabel('True class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = make_cholesky(logvar_ab, cov_ab).detach().cpu().numpy()\n",
    "Cov = L @ L.T\n",
    "inv_stds = np.diag(np.diag(Cov)**(-0.5))\n",
    "Corr = inv_stds @ Cov @ inv_stds\n",
    "\n",
    "plt.title(\"Cholesky Matrix\")\n",
    "plt.imshow(L)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Covariance Matrix\")\n",
    "plt.imshow(Cov)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.imshow(Corr)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Means of last layer's parameters\")\n",
    "plt.hist(mu_ab.detach(), bins=25)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Variances of last layer's parameters\")\n",
    "plt.hist(logvar_ab.exp().detach(), bins=25)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Correlations of last layer's parameters\")  # corr coefficients between pairs of DIFFERENT variables\n",
    "correlations = Corr[~np.eye(Corr.shape[0], dtype=bool)]\n",
    "plt.hist(correlations, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:main]",
   "language": "python",
   "name": "conda-env-main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
